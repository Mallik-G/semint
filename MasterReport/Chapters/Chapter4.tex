% Chapter Template

\chapter{SparkSQL Server in Practice: Scan Sharing} % Main chapter title

\label{Chapter4} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 4. \emph{SparkSQL Server in Practice: Scan Sharing}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Scan Sharing}

In large data warehouses, companies, users submit their queries at the same period of time. There are some files which are "hotter" since they are used more frequently than the others, then reading those files multiple times from disk can be redundant by reading each file only one time and many others users can use them. Sharing scan does not only help to reduce the reading cost but also the communication cost, then the total execution time of the queries is reduced.\\

We embed the sharing scan technique into our framework by using two techniques, one from MRShare works and the other is a feature of Spark itself. Both techniques are easily to embed into our system, this proves the generalization and extensibility of our framework.

%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\subsection{Current State-of-the-Art}

As described before,  on Apache Spark, there is no published work on Work Sharing. The current State-of-the-art of Scan Sharing on Apache Hadoop MapReduce is MRShare.\\
MRShare is a sharing framework, which provides two techniques of sharing: map input sharing and map output sharing. The idea of MRShare is group the jobs which can share map input or map output into groups, with the constrains of maximizing the total saving.\\

Grouping jobs into one is not always beneficial since the reading cost is decreased but the amount of data shuffled over networks is increased. Using dynamic programming and a cost model, MRShare evalutates possible groups and decides which groups give it the maximum saving.\\
The technique that MRShare uses to merge multiple jobs into one called simultaneous pipeline. It attaches a tagging in each tuple to identify which job it belongs to.\\
%-----------------------------------
%	SUBSECTION 2
%-----------------------------------

\subsection{New Approach for Scan Sharing on Apache Spark}

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

There is a fancy feature on Apache Spark which is Caching. This feature utilizes the memory storage to keep data in memory and uses it multiple times. When multiple jobs in a Spark Application using the same input file, we can cache the partitions which are hold the input file at the first job. When the second job wants to do some transformations on those partitions, it does not need to recompute from scratch to get them, it just checks if they are in memory or not, if they are in memory, it gets them and applies transformations on them.\\

Caching is a costly operation itself. If the cost of caching is larger than the cost of reading input files multiple times, so caching is not beneficial. In addition, if memory storage of the cluster is not large enough to cache the whole input files, it may write a part of the input files to disk and read them when needed, which also increases the I/O cost.\\

\section{Scan Sharing in SparkSQL Server}
We integrate two techniques of scan sharing: using MRShare and using caching. \\
\textbf{At client side}, since MRShare requires some pre-defined parameters about the jobs, when users submit their Spark applications through the spark-submit command, they need to pass those parameters too.\\

\textbf{At server side}, we only discuss about the components that we need to plug our own implementations for scan sharing:
\begin{itemize}
\item WorkSharing Detector: we need to add our own Rule which is "ScanSharing" to detect which jobs can be shared scan.
\item CostModel: for MRShare, we implement the cost model that was published in the paper. For caching technique, currently, there is no pulished works about cost model relates to caching data in memory so we do not add the cost model for caching.
\item Optimizer: for MRShare, the dynamic programming algorithms are implemented as described in the paper. Since we do not have the cost model for caching technique, currently, we just output the same batch of jobs as the same as the input batch of jobs.
\item Rewriter: We add two Rewrite rules. For MRShare, we add the simultaneous rewrite rule. For caching, we add the caching rewrite rule. The details of the implementations of simultaneous pipeline technique is described in the next section. The rewrite rule for caching is simple since we cache scan RDD of the first job in the batch, then replace the scan RDD of the rest jobs in the batch with the cached scan RDD.
\item PostScheduler: At the moment, we use FIFO strategy for MRShare technique, while in caching technique, it should be noticed that the first job which does the caching of scan RDD needs to be executed first, then we apply FIFO strategy for the rest jobs in the batch.
\end{itemize} 

\section{Implementations}

Caching is a feature of Apache Spark which is widely used. Besides that, the cost model and dynamic programming algorithms are well described in the MRShare paper. In this section, we just discuss about how we implement the simultaneous pipeline technique used in MRShare.

\subsection{Simultaneous Pipeline Technique}

In general, this technique is used when merging multiple jobs read the same input file into one meta job. The technique has been used in Multi Query Optimization of Apache Pig. In MRShare, they proposed this technique on Hadoop MapReduce as an automatic module. On Apache Spark, to the best of our knowledges, all the query optimizations are currently focusing on single query, not multi query. We reintroduce the simultaneous pipeline technique and implement it on Apache Spark.

There are 4 new RDDs that we create to implement the technique.
\begin{itemize}
\item MuxRDD: The MuxRDD is used to buffer the input record and multiplex it into multiple pipelines, which represent multiple jobs.
\item LabellingRDD: The LabellingRDD is used to attach the label into each tuple before it is shuffled over the network. The label is an integer which is attached to the key of the tuple. So, the old tuple [Key, Value] becomes [(Label, Key), Value].
\item DispatchRDD: The DispatchRDD is used to route the tuple to the right pipeline by using the label attached inside it.
\item PullRDD: Because Spark is based on Pull Mechanism, which means the child RDD asks its parent RDDs to give it the tuple. In default, the saveAsTextFile action has the puller to trigger the job. To keep Spark at it is, we create our new puller called PullRDD.
\end{itemize}

We also modify some parts of the shuffle component on Apache Spark:
\begin{itemize}
\item ShuffledRDD: the ShuffledRDD contains only one Aggregator, the object holds the aggregate function. In our case, to merge multiple jobs which have different Aggregators, we create a list of Aggregators to hold their Aggregators.
\item ExternalSorter: before writing to files and shuffling them over the network, Spark does a local aggregation (it is a combiner in Hadoop MapReduce), so we also need to apply the correct aggregate function to the tuple by checking the label attached into it.
\item AppendOnlyMap: when the aggregation happens in the other stage after getting the data over the network (in Hadoop MapReduce, it is the Reduce phase), again, we need to apply the correct aggregate function to the list of tuple by checking the label attached into it.
\end{itemize}

We provide two examples to demonstrate how simultaneous pipeline technique works. Two jobs are described at two figures below. For each figure, on the left are two original DAGs and on the right is the meta DAG which is merged from those two DAGs.
\begin{itemize}
\item Two jobs, each job has only one ShuffleRDD
From figure \ref{fig:simplemetajob}, we can see the metajob is a DAG which is mainly composed from the two original DAGs. The MuxRDD is attached right after the scan RDD, it does the multiplexing the input into two pipelines. Before going to the ShuffleRDD, a LabellingRDD is inserted right before the ShuffleRDD to attach the label into each tuple. In this example, we have two jobs, so the label has two values which is 0 and 1. The ShuffledRDD is modified so it contains two aggregate functions. The aggregation before and after shuffling happens on the correct tuple with the correct aggregate function due to the label attached into each tuple. The DispatchRDD is attached after the ShuffledRDD and routes the correct tuple to the associated pipeline by the label attached into each tuple. The PullRDD is inserted at last to do the pull and trigger the whole job, then saves the output to disk storage.
\item Two jobs, one job has two ShuffleRDDs
The part of DAG1 from the beginning up to the second ShuffledRDD and DAG2 are merged as the same way as the case above. The difference is that the PullRDD just saves a part of data comes from the pipeline of DAG2, PullRDD is also followed by the rest part of DAG1 and saves to disk storage when finishes. The process is described in figure \ref{fig:complexmetajob}

\begin{figure}
\includegraphics[width=\textwidth]{Figures/singlemetajob.jpg}
\label{fig:simplemetajob}
\caption{Transformation process of a simple case}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{Figures/complexmetajob.jpg}
\label{fig:complexmetajob}
\caption{Transformation process of a complex case}
\end{figure}
\end{itemize}

This technique is embedded into Spark Source Code and is an automatic feature of SparkSQL Server. In those two examples, we just discuss about two simple cases that can happen when using MRShare and simultaneous pipeline technique, but the more complex DAGs can also be merged into one meta job automatically.